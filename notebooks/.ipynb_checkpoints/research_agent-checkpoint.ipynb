{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f1edf5-cc84-4b9f-b420-72745aa1621c",
   "metadata": {},
   "source": [
    "## Lets build a sample graph first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48de97c-48db-4e8f-a325-4d90ccfff78a",
   "metadata": {},
   "source": [
    "#### Define Sample State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bcac3f-d563-4a3e-9efe-34e534108c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    step: int = 0\n",
    "    log: list[str] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2527497-19ee-4c26-99f5-516c05e8cf06",
   "metadata": {},
   "source": [
    "#### Create demo nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2884c7f2-f86d-4b48-a2f1-4e0087fafe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "import random\n",
    "\n",
    "def llm_call(state: AgentState):\n",
    "    state.step += 1\n",
    "    state.log.append(\"LLM call executed\")\n",
    "    return state\n",
    "\n",
    "def should_continue(state: AgentState) -> Command[Literal['tool_node', 'compress_research']]:\n",
    "    decision = random.randint(0, 10)\n",
    "    state.log.append(f\"Think node decided with random={decision}\")\n",
    "    if decision > 5:\n",
    "        return Command(goto=\"tool_node\")\n",
    "    else:\n",
    "        return Command(goto=\"compress_research\")\n",
    "\n",
    "def tool_node(state: AgentState):\n",
    "    state.step += 1\n",
    "    state.log.append(\"Tool executed\")\n",
    "    return state\n",
    "\n",
    "def compress_research(state: AgentState):\n",
    "    state.step += 1\n",
    "    state.log.append(\"Research compressed\")\n",
    "    return state\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(AgentState)\n",
    "\n",
    "graph_builder.add_node('llm_call', llm_call)\n",
    "graph_builder.add_node('tool_node', tool_node)\n",
    "graph_builder.add_node('compress_research', compress_research)\n",
    "\n",
    "graph_builder.add_edge(START, \"llm_call\")\n",
    "graph_builder.add_edge(\"tool_node\", \"llm_call\")  # back to LLM after tool\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    'llm_call',\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node\" : \"tool_node\",\n",
    "        \"compress_research\" : \"compress_research\"\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(\"compress_research\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef41bf03-f697-4080-a4ba-055b1002803d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tllm_call(llm_call)\n",
      "\ttool_node(tool_node)\n",
      "\tcompress_research(compress_research)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> llm_call;\n",
      "\tllm_call -.-> compress_research;\n",
      "\tllm_call -.-> tool_node;\n",
      "\ttool_node --> llm_call;\n",
      "\tcompress_research --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "data = graph.get_graph().draw_mermaid()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e72ab-ebf4-4c74-831c-2c62b61cb1e4",
   "metadata": {},
   "source": [
    "#### Now since we have a proper graph flow, lets design the original ResearchState and original graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523414b-a4bb-4c27-b95e-4b16b3ac1c29",
   "metadata": {},
   "source": [
    "### Define the States for the research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03171c84-9055-45c2-a4e0-e2434fa27b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/deep_research/research_scope.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/deep_research/research_scope.py\n",
    "\n",
    "\"\"\"\n",
    "This module includes the Research State and other states requried by the Research Agent\n",
    "\"\"\"\n",
    "\n",
    "from typing_extensions import TypedDict, Sequence, Annotated, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "import operator\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    researcher_messages : Annotated[Sequence[List[BaseMessage]], add_messages]\n",
    "    tool_call_iterations : int\n",
    "    research_topic : str\n",
    "    compressed_research : str\n",
    "    raw_notes : Annotated[List[str], operator.add]\n",
    "\n",
    "class ResearchOutput(TypedDict):\n",
    "    compressed_research : str\n",
    "    raw_notes: Annotated[List[str], operator.add]\n",
    "    researcher_messages : Annotated[Sequence[List[BaseMessage]], add_messages]\n",
    "\n",
    "class LLMOuput(BaseModel):\n",
    "    tool_calls : List[str] = Field(description=\"Which tools to call from the available tools\")\n",
    "    research_message : Optional[str] = Field(description=\"Research message\")\n",
    "    \n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"Schema for webpage content summarization.\"\"\"\n",
    "    summary: str = Field(description=\"Concise summary of the webpage content\")\n",
    "    key_excerpts: str = Field(description=\"Important quotes and excerpts from the content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f7bde7-48fb-46fa-ba0d-cea19505e2a3",
   "metadata": {},
   "source": [
    "#### now lets build tavily tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f8e501-52f7-4cd0-9a4f-b4eddb1491bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/deep_research/tavily.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/deep_research/tavily.py\n",
    "\n",
    "from tavily import TavilyClient\n",
    "from dotenv import load_dotenv\n",
    "from typing_extensions import List, Literal\n",
    "from deep_research.openrouter import init_chat_model\n",
    "from os import getenv\n",
    "from deep_research.prompts import summarize_webpage_prompt\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "load_dotenv()\n",
    "\n",
    "def get_today_str():\n",
    "    \"\"\"return todays date in windows, different method for other os\"\"\"\n",
    "    return datetime.now().strftime(\"%Y -%m -%d\")\n",
    "\n",
    "# define the model\n",
    "summary_model = init_chat_model(model='openai/gpt-oss-120b:free', temperature=0.3, api_key=getenv('OPENROUTER_API_KEY'))\n",
    "\n",
    "# init tavily_client\n",
    "tavily_client = TavilyClient(api_key=getenv('TAVILY_API_KEY'))\n",
    "\n",
    "def tavily_search_multiple(\n",
    "    search_queries : List[str],\n",
    "    max_results: int = 3,\n",
    "    topic : Literal['general', 'finance', 'news', ] = 'general',\n",
    "    include_raw_content: bool =  True\n",
    "):\n",
    "    \"\"\"Perform search using Tavily API for multiple queries.\n",
    "\n",
    "    Args:\n",
    "        search_queries: List of search queries to execute\n",
    "        max_results: Maximum number of results per query\n",
    "        topic: Topic filter for search results\n",
    "        include_raw_content: Whether to include raw webpage content\n",
    "\n",
    "    Returns:\n",
    "        List of search result dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    search_docs = []\n",
    "    for query in search_queries:\n",
    "        result = tavily_client.search(\n",
    "            query=query,\n",
    "            max_results = max_results,\n",
    "            topic=topic,\n",
    "            include_raw_content=include_raw_content\n",
    "        )\n",
    "        search_docs.append(result)\n",
    "    return search_docs\n",
    "\n",
    "def summarize_webpage_content(webpage_content: str) -> str:\n",
    "    \"\"\"Summarize webpage content using the configured summarization model.\n",
    "    \n",
    "    Args:\n",
    "        webpage_content: Raw webpage content to summarize\n",
    "        \n",
    "    Returns:\n",
    "        Formatted summary with key excerpts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up structured output model for summarization\n",
    "        structured_model = summarization_model.with_structured_output(Summary)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = structured_model.invoke([\n",
    "            HumanMessage(content=summarize_webpage_prompt.format(\n",
    "                webpage_content=webpage_content, \n",
    "                date=get_today_str()\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Format summary with clear structure\n",
    "        formatted_summary = (\n",
    "            f\"<summary>\\n{summary.summary}\\n</summary>\\n\\n\"\n",
    "            f\"<key_excerpts>\\n{summary.key_excerpts}\\n</key_excerpts>\"\n",
    "        )\n",
    "        \n",
    "        return formatted_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to summarize webpage: {str(e)}\")\n",
    "        return webpage_content[:1000] + \"...\" if len(webpage_content) > 1000 else webpage_content\n",
    "\n",
    "def deduplicate_search_results(search_results: List[dict]) -> dict:\n",
    "    \"\"\"Deduplicate search results by URL to avoid processing duplicate content.\n",
    "    \n",
    "    Args:\n",
    "        search_results: List of search result dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping URLs to unique results\n",
    "    \"\"\"\n",
    "    unique_results = {}\n",
    "    \n",
    "    for response in search_results:\n",
    "        for result in response['results']:\n",
    "            url = result['url']\n",
    "            if url not in unique_results:\n",
    "                unique_results[url] = result\n",
    "    \n",
    "    return unique_results\n",
    "\n",
    "def process_search_results(unique_results: dict) -> dict:\n",
    "    \"\"\"Process search results by summarizing content where available.\n",
    "    \n",
    "    Args:\n",
    "        unique_results: Dictionary of unique search results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of processed results with summaries\n",
    "    \"\"\"\n",
    "    summarized_results = {}\n",
    "    \n",
    "    for url, result in unique_results.items():\n",
    "        # Use existing content if no raw content for summarization\n",
    "        if not result.get(\"raw_content\"):\n",
    "            content = result['content']\n",
    "        else:\n",
    "            # Summarize raw content for better processing\n",
    "            content = summarize_webpage_content(result['raw_content'])\n",
    "        \n",
    "        summarized_results[url] = {\n",
    "            'title': result['title'],\n",
    "            'content': content\n",
    "        }\n",
    "    \n",
    "    return summarized_results\n",
    "\n",
    "def format_search_output(summarized_results: dict) -> str:\n",
    "    \"\"\"Format search results into a well-structured string output.\n",
    "    \n",
    "    Args:\n",
    "        summarized_results: Dictionary of processed search results\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string of search results with clear source separation\n",
    "    \"\"\"\n",
    "    if not summarized_results:\n",
    "        return \"No valid search results found. Please try different search queries or use a different search API.\"\n",
    "    \n",
    "    formatted_output = \"Search results: \\n\\n\"\n",
    "    \n",
    "    for i, (url, result) in enumerate(summarized_results.items(), 1):\n",
    "        formatted_output += f\"\\n\\n--- SOURCE {i}: {result['title']} ---\\n\"\n",
    "        formatted_output += f\"URL: {url}\\n\\n\"\n",
    "        formatted_output += f\"SUMMARY:\\n{result['content']}\\n\\n\"\n",
    "        formatted_output += \"-\" * 80 + \"\\n\"\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "# ===== RESEARCH TOOLS =====\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def tavily_search(\n",
    "    query: str,\n",
    "    max_results: Annotated[int, InjectedToolArg] = 3,\n",
    "    topic: Annotated[Literal[\"general\", \"news\", \"finance\"], InjectedToolArg] = \"general\",\n",
    ") -> str:\n",
    "    \"\"\"Fetch results from Tavily search API with content summarization.\n",
    "\n",
    "    Args:\n",
    "        query: A single search query to execute\n",
    "        max_results: Maximum number of results to return\n",
    "        topic: Topic to filter results by ('general', 'news', 'finance')\n",
    "\n",
    "    Returns:\n",
    "        Formatted string of search results with summaries\n",
    "    \"\"\"\n",
    "    # Execute search for single query\n",
    "    search_results = tavily_search_multiple(\n",
    "        [query],  # Convert single query to list for the internal function\n",
    "        max_results=max_results,\n",
    "        topic=topic,\n",
    "        include_raw_content=True,\n",
    "    )\n",
    "\n",
    "    # Deduplicate results by URL to avoid processing duplicate content\n",
    "    unique_results = deduplicate_search_results(search_results)\n",
    "\n",
    "    # Process results with summarization\n",
    "    summarized_results = process_search_results(unique_results)\n",
    "\n",
    "    # Format output for consumption\n",
    "    return format_search_output(summarized_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd81b3c-f328-4245-ae3c-4d32247ab213",
   "metadata": {},
   "source": [
    "#### once we have the states defined we not proceed to to building the research agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d960cf-a8d5-4c29-bd3d-f782d0e3be27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'compress_research_system_prompt' from 'deep_research.prompts' (C:\\FOG\\deep_research_opensource\\src\\deep_research\\prompts.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeep_research\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenrouter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SystemMessage\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeep_research\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m research_agent_prompt, compress_research_system_prompt, compress_research_human_message\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AIMessage, ToolMessage, SystemMessage, HumanMessage\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeep_research\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresearch_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResearchState, ResearchOutput, LLMOuput, Summary\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'compress_research_system_prompt' from 'deep_research.prompts' (C:\\FOG\\deep_research_opensource\\src\\deep_research\\prompts.py)"
     ]
    }
   ],
   "source": [
    "from deep_research.openrouter import init_chat_model\n",
    "from langchain_core.messages import SystemMessage\n",
    "from deep_research.prompts import research_agent_prompt, compress_research_human_message, compress_research_system_prompt\n",
    "from langchain_core.messages import AIMessage, ToolMessage, SystemMessage, HumanMessage\n",
    "from deep_research.research_state import ResearchState, ResearchOutput, LLMOuput, Summary\n",
    "from deep_research.tavily import tavily_search\n",
    "from typing_extensions import Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "model = init_chat_model(model='openai/gpt-oss-120b:free', temperature=0.3, api_key=getenv('OPENROUTER_API_KEY'))\n",
    "compress_model = init_chat_model(model='x-ai/grok-4-fast:free', temperature=0.3, api_key=getenv('OPENROUTER_API_KEY')).with_structured_output(ResearchOutput)\n",
    "tools = [tavily]\n",
    "tools_by_name = {tool.name : tool for tool in tools}\n",
    "\n",
    "def get_today_str():\n",
    "    \"\"\"return todays date in windows, different method for other os\"\"\"\n",
    "    return datetime.now().strftime(\"%Y -%m -%d\")\n",
    "\n",
    "def llm_call(state : ResearchState):\n",
    "    \"\"\"\n",
    "    Analyze current state and decide on next actions.\n",
    "    \n",
    "    The model analyzes the current conversation state and decides whether to:\n",
    "    1. Call search tools to gather more information\n",
    "    2. Provide a final answer based on gathered information\n",
    "    \n",
    "    Returns updated state with the model's response.\n",
    "    \"\"\"\n",
    "    structured_model = model.with_structured_output(LLMOuput)\n",
    "    result = structured_model.invoke([\n",
    "        SystemMessage(content=research_agent_prompt.format(date=get_today_str())) + list(state['researcher_messages'])\n",
    "    ])\n",
    "    ai_message = AIMessage(\n",
    "        content=result.research_message or \"\",\n",
    "        additional_kwargs={\n",
    "            \"tool_calls\": llm_result.tool_calls\n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"researcher_messages\" : [ai_message]\n",
    "    }\n",
    "\n",
    "def tool_node(state : ResearchState):\n",
    "    tool_calls = state['researcher_messages'][-1].tool_calls\n",
    "    observations = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool = tools_by_name[tool_call['name']]\n",
    "        observations.append(tool.invoke(tool_call['args']))\n",
    "\n",
    "    tool_outputs = [\n",
    "        ToolMessage(\n",
    "            content=observation,\n",
    "            name=tool_call['name'],\n",
    "            tool_call_id=tool_call['id']\n",
    "        ) for observation, tool_call in zip(observations, tool_calls)\n",
    "    ]\n",
    "\n",
    "    return {'researcher_messages' : tool_outputs}\n",
    "\n",
    "def should_continue(state : ResearchState) -> Literal['llm_call','compress_research']:\n",
    "    last_message = state['researcher_messsages'][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return 'tool_calls'\n",
    "    return 'compress_research'\n",
    "\n",
    "def compress_research(state : ResearchState) -> dict:\n",
    "    system_message = compress_research_system_prompt.format(date=get_today_str())\n",
    "    messages = [SystemMessage(content=system_message)] + state.get(\"researcher_messages\", []) + [HumanMessage(content=compress_research_human_message)]\n",
    "    response = compress_model.invoke(messages)\n",
    "    raw_notes = [\n",
    "        str(m.content) for m in filter_messages(\n",
    "            state[\"researcher_messages\"], \n",
    "            include_types=[\"tool\", \"ai\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"compressed_research\": str(response.content),\n",
    "        \"raw_notes\": [\"\\n\".join(raw_notes)]\n",
    "    }\n",
    "\n",
    "graph_builder = StateGraph(ResearchState, output_schema=ResearchOutput)\n",
    "\n",
    "graph_builder.add_node('llm_call', llm_call)\n",
    "graph_builder.add_node('tool_node', tool_node)\n",
    "graph_builder.add_node('compress_research', compress_research)\n",
    "\n",
    "graph_builder.add_edge(START, \"llm_call\")\n",
    "graph_builder.add_edge(\"tool_node\", \"llm_call\")  # back to LLM after tool\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    'llm_call',\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node\" : \"tool_node\",\n",
    "        \"compress_research\" : \"compress_research\"\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(\"compress_research\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "## we have stopped before creation of the the tool node and now will continue to create tool node from monday, we build the full agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8316096-a6fd-40dd-bbbe-b3dfd3d09325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
